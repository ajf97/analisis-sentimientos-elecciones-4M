# -*- coding: utf-8 -*-
# ---
# jupyter:
#   jupytext:
#     formats: ipynb,py:percent
#     text_representation:
#       extension: .py
#       format_name: percent
#       format_version: '1.3'
#       jupytext_version: 1.11.2
#   kernelspec:
#     display_name: Python 3
#     language: python
#     name: python3
# ---

# %% [markdown]
# <center><img src="images/banner.jpg"></center>
#
# <center><h1>An√°lisis de sentimientos de las elecciones a la Asamblea de Madrid 2021</h1></center>
#
#

# %% [markdown]
# ## Introducci√≥n
#
# El pasado martes 4 de mayo se celebraban las elecciones a la Asamblea de Madrid 2021. Estas elecciones son de alguna forma excepcionales y pol√©micas desde el mismo instante de su convocatoria. Esto se debe a la pandemia y a la polarizaci√≥n pol√≠tica de los √∫ltimos a√±os en Espa√±a. As√≠ pues, se han saldado con la victoria de los partidos derecha y la derrota de la izquierda, obligando a dos de los principales candidatos a abandonar la pol√≠tica. Tambi√©n hemos asistido a la desaparici√≥n del partido Ciudadanos de la Asamblea, empeorando a√∫n m√°s su crisis como partido.
#
# Por si fuera poco, la campa√±a electoral se desarroll√≥ bajo un clima de mucha tensi√≥n, ya que algunos pol√≠ticos del pa√≠s recibieron sobres con balas. En las redes sociales, esta polarizaci√≥n se manifiesta m√°s. Es frecuente ver insultos y descalificaciones en la mayor√≠a de los mensajes.
#
# Las elecciones de Madrid pueden afectar de alguna forma a elecciones futuras. Por tanto, es interesante analizar los sentimientos de los usuarios en redes ‚Äîen este caso Twitter‚Äî con el objetivo de ver c√≥mo se desarrolla esta polarizaci√≥n en todo el pa√≠s.
#
# ### üü¢ Objetivo:
#
# > Realizar un an√°lisis de sentimientos en Twitter para ver el desarrollo de la polarizaci√≥n en Espa√±a.
#
# ### üìö Librer√≠as
#
# Para conseguir nuestro objetivo, necesitamos las siguientes librer√≠as:
#
# * **Tweepy**: la usaremos para descargar los tweets utilizando la API de Twitter.
# * **Pandas**: librer√≠a para el an√°lisis de los datos.

# %%
import random
from collections import Counter
from string import punctuation

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from emoji.unicode_codes.es import EMOJI_UNICODE_SPANISH
from nltk import FreqDist, word_tokenize
from nltk.corpus import stopwords
from PIL import Image
from sentiment_analysis_spanish import sentiment_analysis
from wordcloud import WordCloud

# Tienes que descargarte las stopwords primero usando nltk.download()

# %% [markdown]
# ## TODO: Extracci√≥n de tweets

# %% [markdown]
# ## üßπ Limpieza de datos
#
# Una vez extra√≠dos los tweets en un archivo CSV, el siguiente paso consiste en eliminar aquellos datos que no son importantes para el an√°lisis. Para ello, vamos a usar la librer√≠a `Pandas`, que nos permitir√° gestionar los tweets en tablas llamadas *Dataframes*.

# %%
# Importamos los datos a un Dataframe
data = pd.read_csv("data/data.csv", index_col="ID")

data.head()

# %% [markdown]
# A continuaci√≥n, vamos a hacer un an√°lisis exploratorio para ver los datos que disponemos. Comprobaremos tambi√©n si tenemos datos nulos y duplicados.

# %%
# Mostramos las filas y columnas del dataframe
data.shape

# %%
# Comprobamos valores nulos
data.isna().any()

# %%
# Eliminar la cadena 'RT' en los tweets para quedarnos con la informaci√≥n relevante
data["Tweets"] = data["Tweets"].str.replace(
    r"RT.*:",
    "",
    regex=True,
)

# %%
# Eliminar saltos de l√≠nea y retorno para tenerlo todo en una frase
data["Tweets"] = data["Tweets"].str.replace("\n", "")
data["Tweets"] = data["Tweets"].str.replace("\r", "")


# %%
# Eliminar etiqueta #Elecciones4M
data["Tweets"] = data["Tweets"].str.replace("#Elecciones4M", "")

# %%
# Eliminar links
links_index = data[data["Tweets"].str.contains(r"//t\.co.*", regex=True)].index
data = data.drop(links_index)

# %% [markdown]
# Seguimos haciendo limpieza de los datos, eliminando informaci√≥n que no es relevante para el an√°lisis.

# %%
# Seguimos eliminando links
links_index = data[data["Tweets"].str.contains("//t.c‚Ä¶")].index
data = data.drop(links_index)
data.shape

# %%
# Eliminamos filas con ruido
data = data.drop(
    [
        1389714946599137283,
        1389714670081265669,
        1389714532499697667,
        1389715033786040322,
        1389714949367336965,
        1389714934095880192,
        1389714915737448451,
        1389714894241554438,
        1389714495413555202,
        1389715170969231361,
        1389715027356266502,
    ]
)

# %%
# Eliminar duplicados
data = data.drop_duplicates(subset=["Tweets"])

# %%
# Exportamos los datos preparados
data.to_csv("data/data_prepared.csv")

# %% [markdown]
# ## Procesamiento del texto
#
# Despu√©s de limpiar los datos, tenemos que hacer el procesamiento del texto.
#

# %%
spanish_stopwords = stopwords.words("spanish")

# Eliminamos signos puntuaci√≥n, n√∫meros y emojis
emoji_list = list(EMOJI_UNICODE_SPANISH.values())

non_words = list(punctuation)
non_words.extend(["¬ø", "?", "‚Ä¶", "", "‚Äú", "¬´" "¬ª", "‚Ä¢", "¬°", "‚Äù"])
non_words.extend(map(str, range(10)))
emoji_list.extend(["üó≥Ô∏è", "üó£Ô∏è", "üó≥", "‚û°Ô∏è"])
non_words.extend(emoji_list)

# %%

# Tokenizamos todas las palabras

tweets_text = data["Tweets"]
tweets_text = tweets_text.apply(word_tokenize)

tweets_text = tweets_text.to_list()
tweets_text = [j for i in tweets_text for j in i]
# %%

# Funci√≥n para eliminar signos de puntuaci√≥n y emojis


def drop_nonwords(tokens, non_words):
    tk_nonwords = []

    for word in tokens:
        text = "".join([letter for letter in word if letter not in non_words])
        if text != "":
            tk_nonwords.append(text.lower())

    return tk_nonwords


tweets_text = drop_nonwords(tweets_text, non_words)

# Eliminamos las stopwords
tweets_text = [word for word in tweets_text if word not in spanish_stopwords]

# %%

# Calculamos la frecuencia de palabras
frecuency = FreqDist(tweets_text)
print(frecuency.most_common(100))


# %%
# Importamos la imagen del logo de Twitter
twitter_logo = np.array(Image.open("images/twitter_logo.png"))

figure = plt.figure()
figure.set_figwidth(14)
figure.set_figheight(18)

plt.imshow(twitter_logo)
plt.axis("off")
plt.show()


# %%
# Funci√≥n para generar el color azul de la nube de tweets
def blue_color(
    word,
    font_size,
    position,
    orientation,
    ramdom_state=None,
    **kwargs,
):
    return "hsl(210, 100%%, %d%%)" % random.randint(50, 70)


# %%
tweets_long_string = tweets_text
tweets_long_string = " ".join(tweets_long_string)


# %%
twitter_wc = WordCloud(
    background_color="white",
    max_words=1500,
    mask=twitter_logo,
)

# Generamos la nube de palabras
twitter_wc.generate(tweets_long_string)

# Mostramos por pantalla la nube de palabras
figure = plt.figure()
figure.set_figwidth(14)
figure.set_figheight(18)

plt.imshow(
    twitter_wc.recolor(color_func=blue_color, random_state=3),
    interpolation="bilinear",
)
plt.axis("off")
plt.show()


# %% [markdown]
# # An√°lisis de sentimientos

# %%
# A√±adimos nueva columna al DataFrame con los valores de probabilidad

sentiment = sentiment_analysis.SentimentAnalysisSpanish()
data["sentiment_probability"] = data["Tweets"].apply(sentiment.sentiment)

data.head()


# %%
def probability_labeler(probability):
    if probability > 0.5:
        return "Positive"
    elif probability == 0.5:
        return "Neutral"
    elif probability < 0.5:
        return "Negative"


# A√±adimos otra columna con la etiqueta de los sentimientos

data["sentiment"] = data["sentiment_probability"].apply(probability_labeler)

data.head()

# %%
print(Counter(data["sentiment"].to_list()))
# %%

# Creamos un nuevo dataframe para el gr√°fico de barras

bar_chart = (
    data["sentiment"]
    .value_counts()
    .rename_axis("Sentiment")
    .to_frame("Total Tweets")
    .reset_index()
)

bar_chart
# %%

# Mostramos el gr√°fico de barras

bar = plt.bar(
    bar_chart["Sentiment"],
    bar_chart["Total Tweets"],
    align="center",
    alpha=0.5,
)

bar[0].set_color("r")
bar[1].set_color("b")

plt.ylabel("Total Tweets")
plt.title("Distribution of Sentiments Results")
plt.show()
